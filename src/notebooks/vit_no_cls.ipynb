{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import clip\n",
    "\n",
    "PWD = os.chdir(os.path.join(os.getcwd(), '..'))\n",
    "PWD = os.getcwd()\n",
    "print(f'PWD is {PWD}')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VisionTransformerExtraHead\n",
    "from data import transform_val\n",
    "from utils import visualize_attention_patches, load_pretrained_vit, stitch_images, get_top_n_results\n",
    "from _types import (\n",
    "    PretrainedViTNames, \n",
    "    vit_extended_same_norm_masked_28_args_16_heads_512_width as vit_multimodal_patch_args, \n",
    "    vit_extended_28_args_16_heads_512_width as vit_no_cls_args\n",
    ")\n",
    "\n",
    "pretrained_clip_vit = load_pretrained_vit(PretrainedViTNames.vit_b_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenames = glob('xxx/*.jpg')\n",
    "len(imagenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store embeddings and attention probabilities for the pretrained ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_pretrained, image_attn_probs_pretrained = [],[]\n",
    "\n",
    "for imagename in imagenames:\n",
    "    image = Image.open(imagename).convert('RGB')\n",
    "    with torch.no_grad():\n",
    "        y_pretrained, attn_probs_pretrained = pretrained_clip_vit(transform_val(image).unsqueeze(0))\n",
    "        image_embeds_pretrained.append(y_pretrained.squeeze())\n",
    "        image_attn_probs_pretrained.append(attn_probs_pretrained[0,0,1:].view(7,7))\n",
    "\n",
    "image_embeds_pretrained = torch.stack(image_embeds_pretrained)\n",
    "image_embeds_pretrained_unit = image_embeds_pretrained / image_embeds_pretrained.norm(dim=-1, keepdim=True)\n",
    "image_attn_probs_pretrained = torch.stack(image_attn_probs_pretrained)\n",
    "image_embeds_pretrained.shape, image_attn_probs_pretrained.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store embeddings and attention probabilities for the newly trained ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_no_cls_model = VisionTransformerExtraHead(**vit_no_cls_args.model_dump())\n",
    "vit_no_cls_model.scale = None\n",
    "\n",
    "CKPT_DIR = os.path.join(PWD, '..', 'checkpoints')\n",
    "CKPT_FILE = os.path.join(CKPT_DIR, 'checkpoint_epoch24_vit_extended_dim_2024-04-11_19-18-30.pt')\n",
    "state_dict = torch.load(CKPT_FILE, map_location='cpu')\n",
    "vit_no_cls_model.load_state_dict(state_dict['model_state_dict'])\n",
    "_ = vit_no_cls_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_student, image_attn_probs_student = [],[]\n",
    "\n",
    "for imagename in imagenames:\n",
    "    image = Image.open(imagename).convert('RGB')\n",
    "    with torch.no_grad():\n",
    "        y_student, attn_probs_student = vit_no_cls_model(transform_val(image).unsqueeze(0), same_norm=False)\n",
    "        image_embeds_student.append(y_student.squeeze())\n",
    "        image_attn_probs_student.append(attn_probs_student[0].view(8,8))\n",
    "\n",
    "image_embeds_student = torch.stack(image_embeds_student)\n",
    "image_embeds_student_unit = image_embeds_student / image_embeds_student.norm(dim=-1, keepdim=True)\n",
    "image_attn_probs_student = torch.stack(image_attn_probs_student)\n",
    "image_embeds_student.shape, image_attn_probs_student.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and compare attention probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attn_probs(index:int) -> Image.Image:\n",
    "    image = Image.open(imagenames[index]).convert('RGB').resize((224,224))\n",
    "    student_probs = image_attn_probs_student[index]\n",
    "    left_image = visualize_attention_patches(student_probs, image)\n",
    "    student_probs = torch.nn.functional.interpolate(student_probs[None, None, ...], (7,7)).squeeze()\n",
    "    middle_image = visualize_attention_patches(student_probs, image)\n",
    "    right_image = visualize_attention_patches(image_attn_probs_pretrained[index], image)\n",
    "    return stitch_images([left_image, middle_image, right_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4372\n",
    "compare_attn_probs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_clip_model, _ = clip.load('ViT-B/32', device=DEVICE)\n",
    "\n",
    "def get_text_vector(text: str) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        tokens = clip.tokenize([text]).to(DEVICE)\n",
    "        return pretrained_clip_model.encode_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search(query:str, n:int=5) -> Image.Image:\n",
    "    text_vector = get_text_vector(query)\n",
    "    top_n_scores, top_n_indices = get_top_n_results(text_vector, image_embeds_student_unit, n=n)\n",
    "    print(f'Top n indices for student model: {top_n_indices}')\n",
    "    images = [ Image.open(imagenames[i]).convert('RGB').resize((224,224)) for i in top_n_indices ]\n",
    "    top_image = stitch_images(images)\n",
    "    top_n_scores, top_n_indices = get_top_n_results(text_vector, image_embeds_pretrained_unit, n=n)\n",
    "    print(f'Top n indices for pretrained model: {top_n_indices}')\n",
    "    images = [ Image.open(imagenames[i]).convert('RGB').resize((224,224)) for i in top_n_indices ]\n",
    "    bottom_image = stitch_images(images)\n",
    "    return stitch_images([top_image, bottom_image], horizontal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'a dog jumping'\n",
    "compare_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-patch-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
