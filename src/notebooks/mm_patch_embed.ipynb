{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import clip\n",
    "\n",
    "PWD = os.chdir(os.path.join(os.getcwd(), '..'))\n",
    "PWD = os.getcwd()\n",
    "print(f'PWD is {PWD}')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VisionTransformerExtraHead\n",
    "from data import transform_val\n",
    "from utils import visualize_attention_patches, load_pretrained_vit, stitch_images, get_top_n_results\n",
    "from _types import (\n",
    "    PretrainedViTNames, \n",
    "    vit_extended_same_norm_masked_28_args_16_heads_512_width as vit_multimodal_patch_args, \n",
    "    vit_extended_28_args_16_heads_512_width as vit_no_cls_args\n",
    ")\n",
    "\n",
    "pretrained_clip_vit = load_pretrained_vit(PretrainedViTNames.vit_b_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenames = glob('xxx/*.jpg')\n",
    "len(imagenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store embeddings and attention probabilities for the pretrained ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_pretrained, image_attn_probs_pretrained = [],[]\n",
    "\n",
    "for imagename in imagenames:\n",
    "    image = Image.open(imagename).convert('RGB')\n",
    "    with torch.no_grad():\n",
    "        y_pretrained, attn_probs_pretrained = pretrained_clip_vit(transform_val(image).unsqueeze(0))\n",
    "        image_embeds_pretrained.append(y_pretrained.squeeze())\n",
    "        image_attn_probs_pretrained.append(attn_probs_pretrained[0,0,1:].view(7,7))\n",
    "\n",
    "image_embeds_pretrained = torch.stack(image_embeds_pretrained)\n",
    "image_embeds_pretrained_unit = image_embeds_pretrained / image_embeds_pretrained.norm(dim=-1, keepdim=True)\n",
    "image_attn_probs_pretrained = torch.stack(image_attn_probs_pretrained)\n",
    "image_embeds_pretrained.shape, image_attn_probs_pretrained.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store embeddings and attention probabilities for the student ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_student = VisionTransformerExtraHead(**vit_multimodal_patch_args.model_dump())\n",
    "\n",
    "CKPT_DIR = os.path.join(PWD, '..', 'checkpoints')\n",
    "CKPT_FILE = os.path.join(CKPT_DIR, 'checkpoint_epoch31_vit_extended_dim_same_norm_attn_mask_2024-04-27_20-29-33.pt')\n",
    "state_dict = torch.load(CKPT_FILE, map_location='cpu')\n",
    "vit_student.load_state_dict(state_dict['model_state_dict'])\n",
    "_ = vit_student.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store all patch embeddings first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_patch_embeds_student, image_attn_probs_student = [], []\n",
    "for imagename in imagenames:\n",
    "    image = Image.open(imagename).convert('RGB')\n",
    "    x = transform_val(image).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        y, attn = vit_student(x, return_all_embeds=True)\n",
    "        image_patch_embeds_student.append(y.squeeze())\n",
    "        image_attn_probs_student.append(attn.squeeze())\n",
    "\n",
    "image_patch_embeds_student = torch.stack(image_patch_embeds_student)\n",
    "image_attn_probs_student = torch.stack(image_attn_probs_student)\n",
    "\n",
    "image_patch_embeds_student.shape, image_attn_probs_student.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then calculate the image embedding from patch embeddings for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_student = (image_attn_probs_student.unsqueeze(1) @ image_patch_embeds_student).squeeze(1)\n",
    "image_embeds_student_unit = image_embeds_student / image_embeds_student.norm(dim=-1, keepdim=True)\n",
    "image_embeds_student.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize similarity with text prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_clip_model, _ = clip.load('ViT-B/32', device=DEVICE)\n",
    "\n",
    "def get_text_vector(text: str) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        tokens = clip.tokenize([text]).to(DEVICE)\n",
    "        return pretrained_clip_model.encode_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_patch_similarities(query:str, index:int):\n",
    "    # (1,dim) @ (dim,64) -> (1,64) -> 64\n",
    "    dot_products = (get_text_vector(query).float() @ image_patch_embeds_student[index].t()).squeeze(0)\n",
    "    return dot_products.mul(2).softmax(-1).view(8,8)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1937\n",
    "image = Image.open(imagenames[index]).convert('RGB').resize((224,224))\n",
    "query = 'green shirt'\n",
    "visualize_attention_patches(calculate_patch_similarities(query, index), image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare search results with pretrained CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search(query:str, n:int=5) -> Image.Image:\n",
    "    text_vector = get_text_vector(query)\n",
    "    top_n_scores, top_n_indices = get_top_n_results(text_vector, image_embeds_student_unit, n=n)\n",
    "    print(f'Top n indices for student model: {top_n_indices}')\n",
    "    images = [ Image.open(imagenames[i]).convert('RGB').resize((224,224)) for i in top_n_indices ]\n",
    "    top_image = stitch_images(images)\n",
    "    top_n_scores, top_n_indices = get_top_n_results(text_vector, image_embeds_pretrained_unit, n=n)\n",
    "    print(f'Top n indices for pretrained model: {top_n_indices}')\n",
    "    images = [ Image.open(imagenames[i]).convert('RGB').resize((224,224)) for i in top_n_indices ]\n",
    "    bottom_image = stitch_images(images)\n",
    "    return stitch_images([top_image, bottom_image], horizontal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_search('rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare attentions with pretrained CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attn_probs(index:int) -> Image.Image:\n",
    "    image = Image.open(imagenames[index]).convert('RGB').resize((224,224))\n",
    "    student_probs = image_attn_probs_student[index].view(8,8)\n",
    "    left_image = visualize_attention_patches(student_probs, image)\n",
    "    student_probs = torch.nn.functional.interpolate(student_probs[None, None, ...], (7,7)).squeeze()\n",
    "    middle_image = visualize_attention_patches(student_probs, image)\n",
    "    right_image = visualize_attention_patches(image_attn_probs_pretrained[index], image)\n",
    "    return stitch_images([left_image, middle_image, right_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_attn_probs(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-patch-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
